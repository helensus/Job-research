---
title: "Job research about data scientists"
author: "Haoran Su"
date: "12/13/2020"
output:
  revealjs::revealjs_presentation:
    theme: solarized
    highlight: pygments
    css: reveal.css
    center: false
    transition: slide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(magrittr)
library(stringr)
library(tidytext)
library(ggplot2)
library(knitr)
```


```{r,echo=FALSE, message=FALSE, warning=FALSE}
list_org <- read.csv("listings2.csv")

## tidy text with unnest_tokens(), take each word into a token
data(stop_words)
tidy_list<- list_org%>% unnest_tokens(word,description) %>%
  anti_join(stop_words) #remove stop words
```

# Introduction
This project is about utilizing text mining in R. Since we are going to find a job of data scientist or statistical job, it is important for us to know about what features the interviewers are looking for. Data is crawled from Indeed website that is for job listings.(indeed link: https://www.indeed.com/)

Here I mainly focus on the following 3 questions:

1.How many job opportunities area there for data scientists and statistical analysists?

2.What skills do the positions require?

3.Are there any similarities and differences between the descriptions of jobs in two different areas?

## Data crawled

Here is a script from the data crawled.
```{r echo=FALSE}
kable(list_org[1:5,2:5])
```



# Job Distribution
## List the top 10 states with the most job positions of data scientists
<font size ="4">The plot shows that MA has the most positions of data scientists, which is not a display of the whole job listing.The reason of having Masachussette ranking 1st might for the location of crawling is at Boston, MA. So my analysis will turn to analysis the job opportunities near my current coordinate.</font>

```{r,echo=FALSE, message=FALSE, warning=FALSE, fig.height=2.5,fig.width=4}
#area
cityjob<-list_org %>% 
  separate(location, into=c("area","state"),sep=",") %>%
  filter(!is.na(state)) %>%
  group_by(state,area)%>%summarise(count = n())%>%arrange(desc(count))
kable(cityjob[2:6,])
## List the top 10 states with the most job positions of data scientists
list_org %>% 
  separate(location, into=c("area","state"),sep=",") %>%
  filter(!is.na(state)) %>%
  count(state, sort=TRUE) %>% 
  mutate(state= reorder(state,n)) %>%
  head(10) %>%
  ggplot(aes(state,n)) +
  geom_col() +
  labs(y=NULL)
```

## List the top 10 companies with the most job positions of data scientists
```{r,echo=FALSE, message=FALSE, warning=FALSE, fig.height=4}
companyjob<-list_org %>%
  filter(!is.na(company)) %>%
  group_by(company)%>%summarize(count = n())%>%arrange(desc(count))
kable(companyjob[2:6,])

list_org %>%
  filter(!is.na(company)) %>%
  count(company, sort=TRUE) %>% 
  mutate(company= reorder(company,n)) %>%
  head(10) %>%
  ggplot(aes(company,n)) +
  geom_col() +
  labs(y=NULL) + ylab("Position") +xlab("Company")+
  theme(axis.text.x = element_text(size = 12, color = "darkblue", vjust = 0.5, hjust = 0.5, angle = 45))
```


# Word frequency in Description
<font size ="5">Looking at the description of jobs, I want to know "what skills do the positions require".</font>

## Word Cloud
```{r,echo=FALSE, message=FALSE, warning=FALSE,fig.width=5}
library(wordcloud)
freq<- list_org %>% unnest_tokens(word,description) %>%
  anti_join(stop_words)  %>%
  mutate(word = str_extract(word, "[a-z']+"))   %>% count(word)
#pal <- choose_palette()
wordcloud(freq$word,freq$n,min.freq = 3,max.words=200, random.order=FALSE, rot.per=0.2,colors=brewer.pal(2, "Dark2")) 
```

## The most 10 common words in description 

```{r,echo=FALSE, message=FALSE, warning=FALSE}
# plot the most 10 common words in description
tidy_list %>%
  count(word, sort=TRUE) %>% 
   mutate(word= reorder(word,n)) %>%
    filter(!is.na(word)) %>%
    head(10) %>%
     ggplot(aes(n, word)) +
     geom_col() +
     labs(y=NULL)
```

# Words appear together
<font size ="5">Some words may come together. The following plot shows common bigrams of words, showing those that occurred more than 3 times and where neither word was a stop word. </font>
```{r,echo=FALSE, message=FALSE, warning=FALSE}
library(ggraph)  #devtools::install_github('thomasp85/ggraph')
library(igraph)
count_bigrams <- function(dataset) {
  dataset %>%
    unnest_tokens(bigram, description, token = "ngrams", n = 2) %>%
    separate(bigram, c("word1", "word2"), sep = " ") %>%
    filter(!word1 %in% stop_words$word,
           !word2 %in% stop_words$word) %>%
    count(word1, word2, sort = TRUE)
}

visualize_bigrams <- function(bigrams) {
  set.seed(2016)

  bigrams %>%
    ggraph (layout = "fr") +
    geom_edge_link() +
    geom_node_point(color = "lightpink", size = 5) +
    geom_node_text(aes(label = name), vjust = 1, hjust = 1)
}
#take it to the text-list_org
list_org %>%
  count_bigrams()%>%
  filter(n > 3 & !is.na(word1) & !is.na(word2)) %>%
  graph_from_data_frame() %>%
  visualize_bigrams()
```

# The descriptions of jobs in two different areas
<font size ="5">As my location is at Boston, I want to look at the job descriptions of the areas nearby. The plot shows the comparison the word frequencies of Boston, Cambridge and Framingham. </font>
```{r,echo=FALSE, message=FALSE, warning=FALSE,fig.height=6}
#plot of word frequencies: boston compare with Cambridge and Framingham
#take out the lines needed
location_compare <-tidy_list  %>%
  separate(location, into=c("area","state"),sep=",") %>%
  filter(!is.na(area)) %>%
  mutate(word = str_extract(word, "[a-z']+")) %>%
  filter(area =="Boston"|area =="Framingham"|area=="Cambridge") %>%
  count(area, word) %>%
  group_by(area)%>%
  mutate(proportion= n/ sum(n)) %>% select(-n)%>%
  spread(area, proportion) %>%
  gather(area,proportion, `Framingham`:`Cambridge`) 


library(scales)
#plot
ggplot(location_compare, aes(x = proportion, y = `Boston`, 
                      color = abs(`Boston` - proportion))) +
  geom_abline(color = "gray40", lty = 2) +
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  scale_color_gradient(limits = c(0, 0.001), 
                       low = "darkslategray4", high = "gray75") +
  facet_wrap(~area, ncol = 2) +
  theme(legend.position="none") +
  labs(y = "Boston", x = NULL)
```

